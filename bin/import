#!/usr/bin/env ruby

require 'open-uri'
require 'open-uri/cached'
require 'nokogiri'
require 'json'
require 'net/http'
require 'net/http/post/multipart'
require 'byebug'

def parse_page(url)
  posts = []

  loop do
    page = Nokogiri::HTML(URI.open(url))

    page.css('.changelogItem.published').each do |post|
      title = post.css('.title').text.strip
      content = post.css('.content').inner_html.strip
      published_at = post.css('time').attribute('datetime').value

      next if ENV['START_DATE'].present? && Date.parse(published_at) < Date.parse(ENV['START_DATE'])

      doc = Nokogiri::HTML.fragment(content)
      doc.css('img').each do |img|
        original_url = img['src']
        uploaded_image_url = upload_image(original_url)
        img['src'] = uploaded_image_url if uploaded_image_url
      end

      # Clean the content before storing it
      content = clean_content(doc.to_html)

      posts << { title:, content:, published_at: }

      puts "Reading: #{title}"
    end

    next_link = page.css('.pagination a').attr('href')

    break if next_link.nil? # Exit loop when there's no more pages
    url = ENV['SOURCE_URL'] + next_link # Prepare URL for next page
  end

  posts
end

def clean_content(content)
  doc = Nokogiri::HTML.fragment(content)

  # Remove category, beforeCategories and afterCategories elements
  ['.beforeCategories', '.category', '.afterCategories'].each do |selector|
    doc.css(selector).remove
  end

  # Remove leading and trailing <br> elements within each paragraph
  doc.css('p').each do |paragraph|
    first_child = paragraph.children.first
    last_child = paragraph.children.last

    first_child.remove if first_child.name == 'br'
    last_child.remove if last_child.name == 'br'
  end

  # Remove initial whitespace
  while doc.children.first && doc.children.first.text.strip.empty?
    doc.children.first.remove
  end

  # Remove newline characters and non-breaking spaces
  doc.to_s.gsub(/[\n\u00A0]/, "").strip
end

def upload_image(url)
  begin
    url = "https:#{url}" unless url.start_with?('http://', 'https://')

    file_uri = URI.parse(url)
    file_name = File.basename(file_uri.path)
    file = URI.open(url)

    uri = URI("#{ENV['CHANGEPACK_URL']}/api/v1/images")
    http = Net::HTTP.new(uri.host, uri.port)
    http.use_ssl = true
    http.verify_mode = OpenSSL::SSL::VERIFY_NONE # Disable SSL verification

    request = Net::HTTP::Post::Multipart.new(
      uri.path,
      { file: UploadIO.new(file, "image/jpeg", file_name) }
    )
    request['Authorization'] = "Bearer #{ENV['API_TOKEN']}"

    response = http.request(request)
    JSON.parse(response.body)['url'] if response.is_a?(Net::HTTPSuccess)
  rescue => e
    byebug
    puts "Failed to upload image: #{url} due to #{e}"
    nil
  end
end

def send_posts(posts)
  uri = URI("#{ENV['CHANGEPACK_URL']}/api/v1/posts")
  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true
  http.verify_mode = OpenSSL::SSL::VERIFY_NONE # Disable SSL verification

  posts.each do |post|
    request = Net::HTTP::Post.new(uri, 'Content-Type' => 'application/json')
    request['Authorization'] = "Bearer #{ENV['API_TOKEN']}"
    request.body = post.to_json

    response = http.request(request)

    if response.is_a?(Net::HTTPSuccess)
      puts "Imported: '#{post[:title]}'"
    else
      puts "Failed to import: '#{post[:title]}'"
    end
  end
end

# Scrape the posts
posts = parse_page(ENV['SOURCE_URL'])

# After all posts are scraped, send them to API
send_posts(posts)
